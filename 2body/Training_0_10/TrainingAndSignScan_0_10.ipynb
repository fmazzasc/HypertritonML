{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook per fare il Training e il Testing del modello per la classe di centralità 10-40 % per fare il confronto con i risultati dell'analisi standard. Nella prima parte del notebook Training e Testing, nella seconda viene stimata la Significance che si otterrebbe misurando lo yield. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "%run ../Utils/analysis_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preliminary\n",
    "\n",
    "Carico i dati, defisco le variabili su cui fare Training e preparo il Training Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML = pd.read_pickle('../../../HypertritonAnalysis/Dataframes/df_ML.pkl')\n",
    "bkg = df_ML.query('y==0')\n",
    "sig = df_ML.query('y==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([bkg,sig],ignore_index=True)\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_columns = [ 'V0CosPA','ProngsDCA', 'DistOverP','ArmenterosAlpha','NpidClustersHe3','V0pt','TPCnSigmaHe3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "traindata,testdata,ytrain,ytest = train_test_split(df, df['y'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotto le variabili di Training del segnale e del fondo come confronto e la matrice delle correlazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distr(df,training_columns+['InvMass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(df,training_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For using pre-trained models skip to the Testing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=23)\n",
    "scoring = 'auc'\n",
    "early_stopping_rounds = 20\n",
    "num_rounds = 200\n",
    "params_def = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':8,\n",
    "    'eta':0.05,\n",
    "    'gamma':0.7,\n",
    "    'min_child_weight':8,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.9,\n",
    "    'objective':'binary:logistic',\n",
    "    'random_state':42,\n",
    "    'silent':1,\n",
    "    'nthread':4,\n",
    "    'tree_method':'hist',\n",
    "    'scale_pos_weight': 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_params(dtrain,par):\n",
    "    %run analysis_utils.py\n",
    "    gs_dict = {'first_par': {'name': 'max_depth', 'par_values': [i for i in range(2, 10, 2)]},\n",
    "           'second_par': {'name': 'min_child_weight', 'par_values':[i for i in range(0, 12, 2)]},\n",
    "          }\n",
    "    par['max_depth'],par['min_child_weight'],_ = gs_2par(gs_dict, par, dtrain, num_rounds, 42, cv, scoring, early_stopping_rounds)\n",
    "    \n",
    "    gs_dict = {'first_par': {'name': 'subsample', 'par_values': [i/10. for i in range(4, 10)]},\n",
    "           'second_par': {'name': 'colsample_bytree', 'par_values': [i/10. for i in range(8, 10)]},\n",
    "          }\n",
    "    par['subsample'],par['colsample_bytree'],_ = gs_2par(gs_dict, par, dtrain, num_rounds, 42, cv, scoring, early_stopping_rounds)\n",
    "    gs_dict = {'first_par': {'name': 'gamma', 'par_values': [i/10. for i in range(0, 11)]}} \n",
    "    par['gamma'],_ = gs_1par(gs_dict, par, dtrain, num_rounds, 42, cv, scoring, early_stopping_rounds)\n",
    "    gs_dict = {'first_par': {'name': 'eta', 'par_values': [0.1, 0.05, 0.01, 0.005, 0.001]}}\n",
    "    par['eta'],n = gs_1par(gs_dict, par, dtrain, num_rounds, 42, cv, scoring, early_stopping_rounds)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=np.asarray(traindata[training_columns]), label=ytrain, feature_names=training_columns)\n",
    "#n_round = optimize_params(dtrain,params_def)\n",
    "model_cent_integr = xgb.train(params_def, dtrain,num_boost_round=num_rounds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "pickle.dump(model_cent_integr,open(\"../Models/model_0_10.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "If you skip the Training start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    model = model_cent_integr\n",
    "else:\n",
    "    model = pickle.load(open(\"../Models/model_0_10.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(data=testdata[training_columns])\n",
    "y_pred = model.predict(dtest,output_margin=True)\n",
    "plot_roc(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_output_train_test(model, traindata[training_columns], ytrain, testdata[training_columns], ytest, branch_names=training_columns,raw=True,log=True,location=9)\n",
    "plt.ylim([10**-5,10**0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output_train_test(model, traindata[training_columns], ytrain, testdata[training_columns], ytest, branch_names=training_columns,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT Efficiency \n",
    "\n",
    "Calcolo l'efficienza del modello in funzione dello Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testdata.eval('Score = @y_pred',inplace=True)\n",
    "efficiency_array=EfficiencyVsCuts(testdata)\n",
    "plt.figure() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Scan Vs pT\n",
    "\n",
    "Scan della Significance Vs BDT Score negli stessi bin di pT in cui Stefano ha estratto lo yield doppio differenziale per confrontare la massima sign. ottenibile con il BDT con la sign. ottenuta con il metodo standard. Eventi in classe di centralità 10-40%.\n",
    "\n",
    "### Load data 0_10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18r = pd.read_pickle('../../../HypertritonAnalysis/Dataframes/df_Sig_data.pkl')\n",
    "df_18r = df_18r.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SignificanceError(sig,bkg,i):\n",
    "    yield_meas = [2.6*1.e-5,2.6*8.e-6,2.6*4.e-6,2.6*9.e-7]\n",
    "    err_sig=np.sqrt(yield_meas[i])/(yield_meas[i])*sig\n",
    "    err_bkg=sqrt(bkg)\n",
    "    err_sig=sqrt(sig)\n",
    "    err_1=(sqrt(sig+bkg)-sig*(1/(2*sqrt(sig+bkg))))/(sig+bkg)\n",
    "    err_2=sig/(2*(sig+bkg)**(3/2))\n",
    "    return abs(err_1)*err_sig+abs(err_2)*err_bkg\n",
    "\n",
    "\n",
    "def ExpectedSignal(eff_bdt, i):\n",
    "    n_ev = 45070038\n",
    "    yield_meas = [2.6*1e-5,2.6*8e-6,2.6*4e-6,2.6*9e-7] # values taken from S.Trogolo PhD Thesis\n",
    "    eff_V0 = [0.19,0.29,0.38,0.47] # values computed in eff2body.cc macro\n",
    "    dpT = [1,1,1,4]\n",
    "    return int(round(n_ev*yield_meas[i]*dpT[i]*eff_V0[i]*eff_bdt))\n",
    "\n",
    "\n",
    "def SignificanceScan(df, pT_min, pT_max, i_pT,custom=False):    \n",
    "    signal_array = []\n",
    "    significance_array = []\n",
    "    custom_significance_array = []\n",
    "    error_array=[]\n",
    "    score_list = np.linspace(-3,12.5,100)\n",
    "    index = 0\n",
    "    for i in score_list:\n",
    "        df_score = df.query('Score>@i and V0pt>=@pT_min and V0pt<=@pT_max')\n",
    "        counts,bins = np.histogram(df_score['InvMass'],bins=26,range=[2.97,3.05]);\n",
    "        bin_centers = 0.5*(bins[1:]+bins[:-1])\n",
    "        sidemap = (bin_centers<2.9923-3*0.0025) + (bin_centers>2.9923+3*0.0025)\n",
    "        massmap = logical_not(sidemap)\n",
    "        bins_side = bin_centers[sidemap]\n",
    "        counts_side = counts[sidemap]\n",
    "        h, residuals, _, _, _ = polyfit(bins_side,counts_side,2,full=True)\n",
    "        chisq_dof = residuals / (len(bins_side) - 3)\n",
    "        y = polyval(h,bins_side)\n",
    "        signal = ExpectedSignal(efficiency_array[index], i_pT)\n",
    "        bkg = sum(polyval(h,bin_centers[massmap]))\n",
    "        significance = signal/sqrt(signal+bkg+1e-10)\n",
    "        signal_array.append(signal)\n",
    "        error_array.append(SignificanceError(signal,bkg,i_pT))\n",
    "        significance_array.append(significance)\n",
    "        custom_significance = significance*efficiency_array[index]\n",
    "        custom_significance_array.append(custom_significance)\n",
    "        index += 1\n",
    "    significance_array=asarray(significance_array)\n",
    "    error_array=asarray(error_array)\n",
    "        \n",
    "    if custom==True:\n",
    "        max_index = np.argmax(custom_significance_array)\n",
    "    else:\n",
    "        max_index = np.argmax(significance_array)\n",
    "        \n",
    "    max_score = score_list[max_index]\n",
    "    sign = significance_array[max_index]\n",
    "    custom_sign = custom_significance_array[max_index]\n",
    "    ryield = signal_array[max_index]\n",
    "    df_cut = df.query('Score>@max_score and V0pt>=@pT_min and V0pt<=@pT_max')\n",
    "    counts_mc_0 = norm.pdf(bin_centers,loc=2.992,scale=0.0025)\n",
    "    counts_mc = (ryield/sum(counts_mc_0))*counts_mc_0\n",
    "    counts_data,_ = np.histogram(df_cut['InvMass'],bins=26,range=[2.97,3.05]);\n",
    "    h = polyfit(bins_side,counts_data[sidemap],2)\n",
    "    counts_bkg = polyval(h,bin_centers)\n",
    "    counts_tot = counts_bkg+counts_mc\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4)) \n",
    "    axs[0].set_xlabel('Score')\n",
    "    axs[0].tick_params(axis=\"x\", direction=\"in\")\n",
    "    axs[0].tick_params(axis=\"y\", direction=\"in\")\n",
    "    \n",
    "    \n",
    "    if custom==True:\n",
    "        axs[0].set_ylabel('Significance x Efficiency')\n",
    "        axs[0].plot(score_list,custom_significance_array,'b',label='Expected significance')\n",
    "        a=custom_significance_array-error_array*efficiency_array\n",
    "        b=custom_significance_array+error_array*efficiency_array\n",
    "        axs[0].fill_between(score_list,a,b,facecolor='deepskyblue',label=r'$ \\pm 1\\sigma$')\n",
    "        axs[0].grid()\n",
    "        \n",
    "    else:\n",
    "        axs[0].set_ylabel('Significance')\n",
    "        axs[0].plot(score_list,significance_array,'b',label='Expected significance')\n",
    "        a=significance_array-error_array\n",
    "        b=significance_array+error_array\n",
    "        axs[0].fill_between(score_list,a,b,facecolor='deepskyblue',label=r'$ \\pm 1\\sigma$')\n",
    "        axs[0].grid()\n",
    "    \n",
    "    axs[0].legend(loc='upper left')\n",
    "    plt.suptitle(r\"%1.f $ \\leq \\rm{p}_{T} \\leq $ %1.f, Cut Score = %0.2f, Significance/Events = %0.4f$x10^{-7}$, Significance x Efficiency = %0.2f , Raw yield = %0.2f\" %(pT_min,pT_max,max_score,(sign/45070038)*1e7,custom_sign,ryield))\n",
    "    \n",
    "    yerr_data = sqrt(counts_data[sidemap])\n",
    "    yerr_tot = sqrt(counts_tot[massmap])\n",
    "    \n",
    "    axs[1].errorbar(bin_centers[sidemap],counts_data[sidemap],yerr=yerr_data,fmt='.',ecolor='k',color='b',elinewidth=1.,label='Data')\n",
    "    axs[1].errorbar(bin_centers[massmap],counts_tot[massmap],yerr=yerr_tot,fmt='.',ecolor='k',color='r',elinewidth=1.,label='Pseudodata')    \n",
    "    axs[1].plot(bin_centers[sidemap],counts_bkg[sidemap],'g-',label='Background fit')\n",
    "    x=np.linspace(2.9923-3*0.0025,2.9923+3*0.0025,1000)\n",
    "    gaussian_counts=norm.pdf(x,loc=2.992,scale=0.0025)\n",
    "    gaussian_counts=(ryield/sum(counts_mc_0))*gaussian_counts+polyval(h,x)\n",
    "    axs[1].plot(x,gaussian_counts,'y',color='orange',label='Gaussian model')\n",
    "    axs[1].set_xlabel(r\"$m_{\\ ^{3}He+\\pi^{-}}$\")\n",
    "    axs[1].set_ylabel(r\"Events /  $3.6\\ \\rm{MeV}/c^{2}$\") \n",
    "    axs[1].tick_params(axis=\"x\", direction=\"in\")\n",
    "    axs[1].tick_params(axis=\"y\", direction=\"in\")    \n",
    "    axs[1].legend(loc=(0.37,0.47))\n",
    "    plt.ylim(ymin=0)\n",
    "    textstr = '\\n'.join((\n",
    "    r\"%1.f GeV/c $ \\leq \\rm{p}_{T} < $ %1.f GeV/c \" %(pT_min,pT_max,),\n",
    "    r' Significance/Events = %0.4f$x10^{-7}$' % ((sign/45070038)*1e7, )))\n",
    "    props = dict(boxstyle='round',facecolor='white', alpha=0,)\n",
    "    axs[1].text(0.37, 0.95, textstr, transform=axs[1].transAxes,\n",
    "        verticalalignment='top', bbox=props)\n",
    "    plt.show()\n",
    "    plt.savefig('fig{}.pdf'.format(i))\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pT_list = [[2,3],[3,4], [4,5],[5,9]]\n",
    "best_score_list=[]\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.figure();\n",
    "    dtest = xgb.DMatrix(data=df_18r[training_columns],silent=True)\n",
    "    df_18r['Score'] = model.predict(dtest,output_margin=True)\n",
    "    best_score_list.append(SignificanceScan(df_18r,pT_list[i][0],pT_list[i][1],i))\n",
    "    del dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pT_list = [[2,3],[3,4],[4,5],[5,9]]\n",
    "best_score_list=[]\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.figure();\n",
    "    dtest = xgb.DMatrix(data=df_18r[training_columns],silent=True)\n",
    "    df_18r['Score'] = model.predict(dtest,output_margin=True)\n",
    "    best_score_list.append(SignificanceScan(df_18r,pT_list[i][0],pT_list[i][1],i,True))\n",
    "    del dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Test on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_function(x, a, x0, sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestOnData(df,score,pt):\n",
    "    df_score = df.query('Score>@score and V0pt>=@pt[0] and V0pt<=@pt[1]')\n",
    "    counts,bins = np.histogram(df_score['InvMass'],bins=26,range=[2.97,3.05]);\n",
    "    bin_centers = 0.5*(bins[1:]+bins[:-1])\n",
    "    sidemap = (bin_centers<2.9923-3*0.0025) + (bin_centers>2.9923+3*0.0025)\n",
    "    massmap = logical_not(sidemap)\n",
    "    bins_side = bin_centers[sidemap]\n",
    "    counts_side = counts[sidemap]\n",
    "    h = polyfit(bins_side,counts_side,2)\n",
    "    y = polyval(h,bins_side)\n",
    "    counts_bkg = polyval(h,bin_centers[massmap]) \n",
    "    counts_sig=counts[massmap]-counts_bkg\n",
    "    popt, pcov = curve_fit(gauss_function, bin_centers[massmap],counts_sig, p0 = [0, 2.9923, 0.0025])\n",
    "    x=linspace(bin_centers[massmap][0]-0.01,bin_centers[massmap][-1]+0.01,100)\n",
    "    x_bkg=polyval(h,x)\n",
    "    plt.plot(x, gauss_function(x, *popt)+x_bkg,'b-',label='Gaussian model')\n",
    "    plt.plot(bin_centers[massmap],counts[massmap],'r.',label='Signal region')\n",
    "    plt.plot(bin_centers[sidemap],y,'g-',label='Sidebands fit')\n",
    "    plt.plot(bin_centers[sidemap],counts[sidemap],'y.',label='Background rergion')\n",
    "    tot_sig=sum(counts_sig)\n",
    "    tot_bkg=sum(counts_bkg)\n",
    "\n",
    "    plt.xlabel(r\"$m_{\\ ^{3}He+\\pi^{-}}$\")\n",
    "    plt.ylabel(r\"Events /  $3.6\\ \\rm{MeV}/c^{2}$\") \n",
    "    plt.tick_params(axis=\"x\", direction=\"in\")\n",
    "    plt.tick_params(axis=\"y\", direction=\"in\")  \n",
    "    textstr =r\"%1.f GeV/c $ \\leq \\rm{p}_{T} < $ %1.f GeV/c \" %(pt[0],pt[1])\n",
    "    props = dict(boxstyle='round',facecolor='white', alpha=0,)\n",
    "    plt.title(textstr)\n",
    "    plt.legend()\n",
    "    print(\"Significance/Events = \" , tot_sig/sqrt(tot_bkg+tot_sig)/45000000*1e7)\n",
    "    print(\"S/B = \" ,tot_sig/tot_bkg)\n",
    "    print(\"Raw yield = \" , tot_sig)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=xgb.DMatrix(data=df_18r[training_columns],silent=True)\n",
    "df_18r['Score']=model.predict(dm,output_margin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestOnData(df_18r,5,[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestOnData(df_18r,5,[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestOnData(df_18r,4,[4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestOnData(df_18r,4.2,[5,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Vs Pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_cuts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    index=list(np.linspace(-3,12.5,100)).index(best_score_list[i])\n",
    "    eff_cuts.append(efficiency_array[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_presel=np.load(\"../PreSelEfficiency/eff_presel_0_10.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_tot=eff_presel*eff_cuts\n",
    "pt=[2.5,3.5,4.5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pt,eff_tot,'r.')\n",
    "plt.xlabel('pt(Gev/c)')\n",
    "plt.ylabel('Efficiency');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
